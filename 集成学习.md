# 集成学习

对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。

集成学习有两个主要的问题需要解决

1、第一是如何得到若干个个体学习器

2、第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器。

# boosting(样本无放回)
弱学习器之间存在强依赖关系
## adaboost
逐步向前更新弱学习器

1、初始化

2、更新弱学习器权重

   分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越小的弱分类器，预测结果越准确，权重系数越大。
   
3、更新样本权重

   根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器中得到更多的重视。
   
4、迭代，直到弱学习器数达到事先指定的数目

## GBDT
逐步向前减小损失函数

随机森林是通过减少模型**方差**提高性能,GBDT是通过减少模型**偏差**提高性能

# bagging(样本有放回)
弱学习器之间不存在强依赖关系
## 随机森林(可以并行计算)
随机选取样本(m个样本中随机有放回选取m个样本)

随机选取特征

## 结合策略

1、平均法(回归)

2、投票法(分类)

3、 学习法(stacking)
