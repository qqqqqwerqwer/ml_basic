# Xgboost

## 对比GBDT的模型改进

### 算法上的改进
1、弱学习器不仅只支持决策树，还支持其他的算法

2、损失函数加了正则化部分

3、GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。

### 运行效率的优化

1、Boosting算法的弱学习器是没法并行迭代的，但是单个弱学习器里面最耗时的是决策树的分裂过程，XGBoost针对这个分裂做了比较大的并行优化。
对于不同的特征的特征划分点，XGBoost分别在不同的线程中并行选择分裂的最大增益。

2、对训练的每个特征排序并且以块的的结构存储在内存中，方便后面迭代重复使用，减少计算量。
首先默认所有的样本都在右子树，然后从小到大迭代，依次放入左子树，并寻找最优的分裂点。这样做可以减少很多不必要的比较。

### 算法健壮性优化

1、对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。

2、算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。
